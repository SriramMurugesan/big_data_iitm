Consider a file “data.bin” which is formatted as follows: every data record is in the form of pairs of values of the form “column name,column value”, with each pair in its own line. You are asked to write a data processing script using Python that scales with big data. Which of the following represents your approach?

Options :

Since data.bin is compliant with the RFC 4180, use PySpark’s read_csv() to readthe data as is.

Rename the file data.bin to data.csv to make it compliant with RFC 4180 andthen use PySpark’s read_csv() to read the data

The problem cannot be solved since the file cannot be converted to a validformat for reading consistently without additional information

Write PySpark code to read all lines in data.bin, use string split on “,” asdelimiter, and then collect all column names and corresponding values into a RDD for furtherprocessing



The correct approach for processing the data in "data.bin" with PySpark is:

Write PySpark code to read all lines in data.bin, use string split on “,” as delimiter, and then collect all column names and corresponding values into an RDD for further processing.

Explanation:
The file data.bin contains records in the form of column name, column value pairs, which is not in a standard CSV format (since each column name-value pair is on a separate line).
Therefore, PySpark's read_csv() function would not directly work unless the data is structured in a valid CSV format (RFC 4180).
Instead, you need to read the file line by line, split the lines by the comma (,) delimiter, and collect the pairs into an RDD for processing.
This approach leverages PySpark's capabilities for distributed data processing, allowing you to scale the solution to handle big data.