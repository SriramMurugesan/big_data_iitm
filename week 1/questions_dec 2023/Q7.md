An enterprise software designer wants to leverage the best of cloud to minimize the number of administrative overheads associated with her big data pipeline while also getting on-demand scalability without sacrificing flexibility. What option should she choose to best serve these needs?

Options :

Build the data pipeline using Spark, data storage on HDFS, and deploy both onHadoop using IaaS

Build the data pipeline using Python run on Serverless where the data is storedon cloud storage

Build the data pipeline using Spark PaaS option on top of data stored on cloudstorage

Build the data pipeline using MapReduce, data storage on HDFS, and deployboth on Hadoop using IaaS

Answer : "Build the data pipeline using Spark PaaS option on top of data stored on cloud storage."

Build the data pipeline using Spark PaaS option on top of data stored on cloud storage.

Explanation:
Minimizing administrative overheads → A PaaS (Platform as a Service) offering for Spark removes the burden of managing infrastructure, software updates, and configurations, making it easier for the enterprise software designer.
On-demand scalability → Cloud storage and Spark PaaS provide elastic scaling, meaning the system can scale up or down based on workload needs.
Flexibility → Spark supports batch and streaming workloads, making it ideal for big data pipelines while leveraging cloud storage, which is scalable and cost-effective.
Why not the other options?
❌ Spark + HDFS on Hadoop using IaaS → Requires manual cluster management, making it more complex and not fully leveraging the cloud’s benefits.
❌ Python Serverless + Cloud Storage → While serverless reduces admin work, it may lack flexibility for big data processing, and Python alone may not be optimal for large-scale processing compared to Spark.
❌ MapReduce + HDFS on Hadoop using IaaS → MapReduce is slower and less flexible than Spark, and managing Hadoop clusters increases administrative overhead.